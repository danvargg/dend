{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Nanodegree\n",
    "\n",
    "## Capstone Project: Immigration Data in the United States\n",
    "Daniel Vargas\n",
    "March 14, 2020\n",
    "#### Project Summary\n",
    "In this project, we will be looking at the `United States` immigration data, specifically:\n",
    "\n",
    "- Effects of temperature on the volume of travellers\n",
    "- Seasonality of travel\n",
    "- Connections between the volume of travel and the number of entry ports (airports)\n",
    "- Connections between the volume of travel and the demographics of various cities\n",
    "\n",
    "We will be using the following datasets:\n",
    "\n",
    "- `I94 Immigration Data`: data from the `US National Tourism and Trade Office` and includes the contents of the `i94` form on entry to the `United States`\n",
    "- `countries.csv`: table containing country codes used in the dataset\n",
    "- `i94portCodes.csv`: table containing city codes used in the dataset\n",
    "- `World Temperature Data`: dataset from Kaggle including the temperatures of various cities in the world between 1743 and 2013\n",
    "- `U.S. City Demographic Data`: data from `OpenSoft`, containing information about the demographics of all US cities and census-designated places with a population greater or equal to `65,000`. The data comes from the `US Census Bureau's 2015 American Community Survey`\n",
    "- `Airport Code Table`: table of airport codes and corresponding cities\n",
    "\n",
    "#### Project steps:\n",
    "1. Scope the project and gather the data\n",
    "2. Explore and assess the data\n",
    "3. Define the data model\n",
    "4. Run `ETL` to model the data\n",
    "5. Complete project write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import udf, date_add, desc, asc\n",
    "from pyspark.sql.functions import sum as Fsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope the project and gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Scope \n",
    "- Aggregate `I94 immigration data` by destination city (dimension table)\n",
    "- Aggregate `city temperature` by city (dimension table)\n",
    "- Join both datasets on destination city (fact table)\n",
    "- Optimize to query on immigration events to determine if temperature affects the selection of destination cities\n",
    "\n",
    "#### Describe and Gather Data\n",
    "`I94 immigration` attributes:\n",
    "- `i94yr`: 4 digit year\n",
    "- `i94mon`: numeric month\n",
    "- `i94cit`: 3 digit code of origin city\n",
    "- `i94port`: 3 character code of destination USA city\n",
    "- `arrdate`: arrival date in the USA\n",
    "- `i94mode`: 1 digit travel code\n",
    "- `depdate`: departure date from the USA\n",
    "- `i94visa`: reason for immigration\n",
    "\n",
    "`Temperature` data attributes:\n",
    "`AverageTemperature` = average temperature\n",
    "- `City`: city name\n",
    "- `Country`: country name\n",
    "- `Latitude`: latitude\n",
    "- `Longitude`: longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immig_sample = pd.read_csv('immigration_data_sample.csv')\n",
    "df_countryCodes = pd.read_csv('countries.csv')\n",
    "i94portCodes = pd.read_csv('i94portCodes.csv')\n",
    "df_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_airports = pd.read_csv('airport-codes_csv.csv')\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write to parquet, commented to avoid error: alysisException: 'path file:/home/workspace/sas_data already exists.;'\n",
    "# df_immigration.write.parquet(\"sas_data\")\n",
    "# df_immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore and assess the data\n",
    "#### Explore the Data \n",
    "For the I94 immigration data, we want to drop all entries where the destination city code i94port is not a valid value (e.g., XXX, 99, etc) as described in I94_SAS_Labels_Description.SAS. For the temperature data, we want to drop all entries where AverageTemperature is NaN, then drop all entries with duplicate locations, and then add the i94port of the location in each entry.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189472, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only data for the United States\n",
    "df_temperature = df_temperature[df_temperature['Country']=='United States']\n",
    "# Convert the date to datetime objects\n",
    "df_temperature['convertedDate'] = pd.to_datetime(df_temperature.dt)\n",
    "# Remove all dates prior to 1950\n",
    "df_temperature=df_temperature[df_temperature['convertedDate']>\"1950-01-01\"].copy()\n",
    "df_temperature[['City','convertedDate']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove missing values (Africa)\n",
    "df_airports = df_airports[df_airports['iso_country'].fillna('').str.upper().str.contains('US')].copy()\n",
    "\n",
    "# closed airports\n",
    "# No for balloonports, seaplane bases or heliport\n",
    "excludedValues = ['closed', 'heliport', 'seaplane_base', 'balloonport']\n",
    "df_airports = df_airports[~df_airports['type'].str.strip().isin(excludedValues)].copy()\n",
    "\n",
    "# Municipality field non-available for all airports\n",
    "df_airports = df_airports[~df_airports['municipality'].isna()].copy()\n",
    "\n",
    "# U-A seems error correction\n",
    "df_airports['len'] = df_airports[\"iso_region\"].apply(len)\n",
    "# let's remove the codes that are incorrect.\n",
    "df_airports = df_airports[df_airports['len']==5].copy()\n",
    "# finally, let's extract the state code\n",
    "df_airports['state'] = df_airports['iso_region'].str.strip().str.split(\"-\", n = 1, expand = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the to upper case and remove leading and trailing spaces\n",
    "df_demographics.City = df_demographics.City.str.upper().str.strip()\n",
    "# Remove leading / trailing spaces and convert to upper case\n",
    "df_demographics.City = df_demographics.City.str.strip().str.upper()\n",
    "df_immigration.createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT (DISTINCT cicid)\n",
    "FROM immig_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the arrdate field\n",
    "# Compute the arrival dates by adding arrdate to 1960-01-01\n",
    "df_immigration = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM immig_table\")\n",
    "df_immigration.createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Define the data model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Run `ETL` to model the data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Complete project write up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
